{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m-intermediate\", revision='global_step10000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_loss_labelsmoothed(logits, labels, ignore_index=-100, epsilon=0.1):\n",
    "    logits = logits[..., :-1, :].contiguous()\n",
    "    labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    log_probs = -nn.functional.log_softmax(logits, dim=-1)\n",
    "    if labels.dim() == log_probs.dim() - 1:\n",
    "        labels = labels.unsqueeze(-1)\n",
    "\n",
    "    padding_mask = labels.eq(ignore_index)\n",
    "\n",
    "    labels = torch.clamp(labels, min=0)\n",
    "    nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "\n",
    "    smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n",
    "\n",
    "    nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "    smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "\n",
    "    # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\n",
    "    num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "    nll_loss = nll_loss.sum() / num_active_elements\n",
    "    smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "    return (1 - epsilon) * nll_loss + epsilon * smoothed_loss\n",
    "\n",
    "    \n",
    "class MyNet(nn.Module): #t5 сложнее создавать создавать датасет для лм так как много пришлось бы возиться с префиксами\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "#         self.transformer = BloomForCausalLM.from_pretrained(\n",
    "#             \"bigscience/bloom-560m-intermediate\",\n",
    "#             revision=revision,\n",
    "#         )\n",
    "\n",
    "#         self.transformer.resize_token_embeddings(self.transformer.config.vocab_size + 30)\n",
    "        \n",
    "        hid_size = self.transformer.config.hidden_size\n",
    "        self.voc_size = self.transformer.config.vocab_size\n",
    "        \n",
    "        self.early_exits = nn.ModuleList([\n",
    "            nn.Linear(hid_size, self.voc_size) for _ in range((self.transformer.config.n_layer - 1) // 4)\n",
    "        ])\n",
    "        \n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        \n",
    "        hidden_states = output.hidden_states[1:-1]\n",
    "        # hidden_states[idx] -- [bs, seqlen, hid_dim]\n",
    "        \n",
    "        heads_outputs = [\n",
    "            self.early_exits[idx](hidden_states[idx]) \n",
    "            for idx in range(len(self.early_exits))\n",
    "            if (idx + 1) % 3 == 0\n",
    "        ]\n",
    "        print(len(heads_outputs))\n",
    "                \n",
    "        if labels is None:\n",
    "            heads_outputs = [\n",
    "                torch.softmax(head_output, dim=-1) for head_output in heads_outputs\n",
    "            ] # h_os[i][bs][seqlen][tok_num] = P(из i-го слоя на seqlen месте стоит токен tok_num)\n",
    "\n",
    "            return {'head_outputs': heads_outputs, 'last_head': torch.softmax(output.logits, dim=-1)}\n",
    "        \n",
    "        # loss = output.loss\n",
    "        # heads_outputs[num_layers, bs, seqlen, num_tokens]\n",
    "        # labels[bs * seqlen]\n",
    "        \n",
    "        \n",
    "        losses = [\n",
    "            compute_loss_labelsmoothed(head_output, labels)\n",
    "            for head_output in heads_outputs\n",
    "        ] # [num_layers, ]\n",
    "        \n",
    "        losses = torch.stack(losses,)\n",
    "        total_loss = torch.sum(losses)\n",
    "        \n",
    "        heads_outputs = [\n",
    "            torch.softmax(head_output, dim=-1) for head_output in heads_outputs\n",
    "        ] # h_os[i][bs][seqlen][tok_num] = P(из i-го слоя на seqlen месте стоит токен tok_num)\n",
    "\n",
    "        return {\n",
    "            'loss': total_loss, \n",
    "            'head_outputs': heads_outputs,  # [num_layers, bs=1, seq_len, vocab_size] # {token: {layer_num: [probabilities, ... ]}}\n",
    "            'last_head': torch.softmax(output.logits, dim=-1),\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5463ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#имею словарь с вер-ями предсказать правильный токен\n",
    "#дальше выбираю слова-токены и смотрю когда они хорошо предсказыввались а когда нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421dab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "net = MyNet().to('cuda')\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'files/dataset'\n",
    "dataset_cache = 'files/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a9463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import numpy as np\n",
    "\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.en\", cache_dir=dataset_cache)\n",
    "\n",
    "# rand_idx = np.random.choice(np.arange(len(dataset['train'])), size=500_000, replace=False)\n",
    "\n",
    "# # import json\n",
    "# # rand_idx = json.load(open('indices.json', 'r'))\n",
    "\n",
    "# dataset = dataset['train'].select(rand_idx, )\n",
    "\n",
    "# # import json\n",
    "\n",
    "# # json.dump(rand_idx.tolist(), open('indices.json', 'w'),)\n",
    "\n",
    "# def tokenize_data(example):\n",
    "#     return tokenizer(example['text'], max_length=512, truncation=True)\n",
    "\n",
    "# dataset = dataset.map(\n",
    "#     tokenize_data, remove_columns=['text', 'id', 'url', 'title'], batched=True, num_proc=10\n",
    "# )\n",
    "\n",
    "# dataset.save_to_disk(dataset_path)\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766800de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in net.named_parameters():\n",
    "    if 'transformer' in n:\n",
    "        p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b89767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=net,\n",
    "    args=TrainingArguments(\n",
    "        'logs/gpt2',\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        save_total_limit=2, \n",
    "        save_steps=1000,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "    ),\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146c14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
